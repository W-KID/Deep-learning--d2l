{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)#只有张量的requires_grad属性为True，PyTorch 才会在执行反向传播时计算该张量对于损失函数的梯度\n",
    "x.grad\n",
    "#在 PyTorch 中，x.grad 是一个张量（tensor），它保存着张量 x 相对于某个标量损失函数的梯度。\n",
    "#具体来说，如果 y 是一个标量张量，且通过执行反向传播计算出了 y 对于 x 的梯度，那么可以通过 x.grad 获取这个梯度值。\n",
    "# 如果张量 x 的 requires_grad 属性为 False，那么 x.grad 的值为 None，因为 PyTorch 不会对不需要计算梯度的张量进行梯度计算。\n",
    "# 另外，如果需要多次计算梯度，需要在每次计算之前将 x.grad 清零，否则 PyTorch 会累加每次计算的梯度值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=2*torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()  # 调用反向传播函数来自动计算y关于x每个分量的梯度\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#函数y=2*xTx关于x的梯度应该是4x,看是否梯度求对\n",
    "x.grad==4*x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()#清零梯度\n",
    "y=x.sum()#y的表达式从新定义为y=x1+x2+x3+x4\n",
    "y.backward()  # 进行反向传播计算y关于x每个分量的梯度，因为xi梯度是1，所以算出来的梯度向量肯定是[1,1,1,1]\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非标量变量的反向传播\n",
    "#在 PyTorch 中，反向传播算法要求损失函数是一个标量（scalar），因为只有标量才有一个唯一的梯度值。\n",
    "#如果损失函数是一个非标量（如矩阵或张量），那么我们需要先将它转化为一个标量，才能进行反向传播\n",
    "# 常见的方法是对损失函数进行求和或平均，将其变为标\n",
    "x.grad.zero_()\n",
    "y=x*x#得出来的是一个向量，不是标量\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分离计算\n",
    "### 计算图：计算图（computational graph）是指记录了张量之间计算依赖关系的有向无环图。在计算图中，每个节点代表一个张量或者一个操作，每个边代表依赖\n",
    "在 PyTorch 中，每个张量都有一个 grad_fn 属性，表示创建该张量的操作。当我们进行前向传播时，PyTorch 会自动构建计算图，并将计算图中的每个操作关联到其输出张量的 grad_fn 属性上。这样，在进行反向传播时，PyTorch 就可以使用这个计算图来自动计算梯度。\n",
    "有时候，我们希望将某些计算移动到记录的计算图之外，以便能够在反向传播时忽略这些计算，或者用一些其他方式处理它们\n",
    "假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。\n",
    "想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在 PyTorch 中，detach() 方法可以将张量从计算图中分离出来，返回一个新的张量，这个新的张量和原来的张量在数值上是相等的，但是在计算图中已经不再依赖原来的张量。\n",
    "# 因此，对分离后的张量进行操作不会影响原来的张量的梯度计算。\n",
    "x.grad.zero_()\n",
    "y=x*x\n",
    "u=y.detach()#此时u已经从计算图中分离出来\n",
    "z=u*x#此时的u不再是变量而只是一个常量\n",
    "z.sum().backward()\n",
    "x.grad==u#z对x的梯度即是u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad==2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d=f(a)\n",
    "k=f(a)\n",
    "d.backward()\n",
    "a.grad==d/a#在 PyTorch 中，对于一个标量张量，可以使用除法操作得到一个标量值。d/a 的含义是f'(a)，即函数 f 在 a 处的导数\n",
    "# 注意d是做了反向传播的，所以这个d/a才能看作是求d在a处的倒数（特定情形适用）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
